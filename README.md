# experimental_models

* This project aims to test different activation functions to see which performs well with mnist dataset.

I have train model with different activations for epoch of 10.

* Results:


* Tensorflow activations: 

  * relu (train_accuracy:97% , test_accuracy:96% )

  * softmax (train_accuracy:94% , test_accuracy:94% )

* Custom non linear activations:

  * add_c3 (train_accuracy:90% , test_accuracy:92% )

  * sub_c2 (train_accuracy:97% , test_accuracy:96% )

  * mul_c2 (train_accuracy:96% , test_accuracy:96% )

  * div_c3 (train_accuracy:95% , test_accuracy:94% )


Detailed results are in Result.txt file.

In future, 
My plan is write research papers on custom non-linear activation functions.

